{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gliomas project: transform the 3 binary classification tasks into 1 multiclass classification task.\n",
    "\n",
    "**Three binary classification tasks:**\n",
    "- Tumor Grade: GBM vs. LGG;\n",
    "- IDH mutant vs. IDH wildtype.\n",
    "- 1p/19q codeleted vs. 1p/19q intact;\n",
    "\n",
    "\n",
    "**One multiclass classification problem:**\n",
    "- LGG, IDH mutant, 1p/19q codeleted:1\n",
    "- LGG, IDH mutant, 1p/19q non-codeleted:2\n",
    "- LGG, IDH wildtype: 3\n",
    "- GBM, IDH mutant: 4\n",
    "- GBM, IDH wildtype: 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "## For plots\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from utils.myUtils import traversalDir_FirstDir, save_dict, load_dict\n",
    "\n",
    "from mySettings import get_random_seed_list, get_convert_binary_to_multiclass_setting_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to convert three binary labels to one multi-class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caculate_tumor_subtype(data, suffix=\"-predicted\"):\n",
    "    \"\"\"\n",
    "    Define the tumor type according to the different combinations of tumor grade, IDH mutant and 1p/19q codeleted status.\n",
    "    \"\"\"\n",
    "    \n",
    "    if data[\"is_GBM\"+suffix]==0 and data[\"is_IDH_mutant\"+suffix]==1 and data[\"is_1p19q_codeleted\"+suffix]==1:\n",
    "        tumor_subtype_description=\"LGG, IDH mutant, 1p/19q codeleted\"\n",
    "        tumor_subtype=1\n",
    "        \n",
    "    elif data[\"is_GBM\"+suffix]==0 and data[\"is_IDH_mutant\"+suffix]==1 and data[\"is_1p19q_codeleted\"+suffix]==0:\n",
    "        tumor_subtype_description=\"LGG, IDH mutant, 1p/19q non-codeleted\"\n",
    "        tumor_subtype=2\n",
    "        \n",
    "    elif data[\"is_GBM\"+suffix]==0 and data[\"is_IDH_mutant\"+suffix]==0:\n",
    "        tumor_subtype_description=\"LGG, IDH wildtype\"\n",
    "        tumor_subtype=3\n",
    "        \n",
    "    elif data[\"is_GBM\"+suffix]==1 and data[\"is_IDH_mutant\"+suffix]==1:\n",
    "        tumor_subtype_description=\"GBM, IDH mutant\"  \n",
    "        tumor_subtype=4\n",
    "        \n",
    "    elif data[\"is_GBM\"+suffix]==1 and data[\"is_IDH_mutant\"+suffix]==0:\n",
    "        tumor_subtype_description=\"GBM, IDH wildtype\"\n",
    "        tumor_subtype=5\n",
    "    \n",
    "    return tumor_subtype_description, tumor_subtype\n",
    "\n",
    "def get_tumor_subtype_description(data):\n",
    "    \"\"\"\n",
    "    Used to add new columns to describe the tumor_subtype by words;\n",
    "    \"\"\"\n",
    "    tumor_subtype_description, tumor_subtype=caculate_tumor_subtype(data)\n",
    "    \n",
    "    return tumor_subtype_description\n",
    "\n",
    "def get_tumor_subtype(data):\n",
    "    \"\"\"\n",
    "    Used to add new columns to describe the tumor_subtype by number in {1,2,3,4,5}.\n",
    "    \"\"\"\n",
    "    tumor_subtype_description, tumor_subtype=caculate_tumor_subtype(data)\n",
    "    \n",
    "    return tumor_subtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to perform the convertion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_predicted):\n",
    "    \"\"\"\n",
    "    Calcualte the metrics for evaluation.\n",
    "    \"\"\"\n",
    "    \n",
    "    result_metrics={}\n",
    "    result_metrics[\"accuracy\"]=metrics.accuracy_score(y_true, y_predicted)\n",
    "\n",
    "    return result_metrics\n",
    "\n",
    "def plot_confusion_matrix(y_true, predicted, save_results_path):\n",
    "    \"\"\"\n",
    "    Plot the confusion matrix.\n",
    "    \"\"\"\n",
    "    classes = np.unique(y_true)\n",
    "    cm = metrics.confusion_matrix(y_true, predicted, labels=classes) #normalize=\"true\" \n",
    "    \n",
    "    # plot the confusion matrix.\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=plt.cm.Blues, cbar=False) #fmt='.2f' \n",
    "    ax.set(xlabel=\"Pred\", ylabel=\"True\", title=\"Confusion matrix\")\n",
    "    ax.set_xticklabels(labels=classes)\n",
    "    ax.set_yticklabels(labels=classes)\n",
    "    plt.savefig(save_results_path)\n",
    "    plt.show()\n",
    "    \n",
    "def arrange_prediction_metrics(binary_task_path_dict, save_results_basepath, data_folder, multiclass_accuracy):\n",
    "    \"\"\"\n",
    "    Function to arrange the prediction metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\\n==============  {}   =================\".format(data_folder))\n",
    "    save_prediction_metrics_excel=os.path.join(save_results_basepath, \"prediction_metrics-\"+data_folder+\".xlsx\")\n",
    "    \n",
    "    ## read the results from the three binary classification tasks into a data frame list, to preprare for data frame connection.\n",
    "    dataframe_list=[]\n",
    "    for binary_task_name, binary_task_path in binary_task_path_dict.items():\n",
    "        best_model_name=traversalDir_FirstDir(binary_task_path)\n",
    "        assert len(best_model_name)==1\n",
    "        \n",
    "        # read the prediction metrics by the best model;\n",
    "        prediction_metrics_file=os.path.join(binary_task_path, best_model_name[0], data_folder, \"prediction_metrics.txt\")\n",
    "        prediction_metrics=load_dict(prediction_metrics_file)\n",
    "        prediction_metrics_df=pd.DataFrame(prediction_metrics, index=[data_folder])\n",
    "        rename_columns={column: binary_task_name+\"-\"+column for column in prediction_metrics_df.columns}\n",
    "        prediction_metrics_df.rename(columns=rename_columns, inplace=True)\n",
    "        prediction_metrics_df[binary_task_name+\"-best_model_name\"]=best_model_name[0]\n",
    "        \n",
    "        # append the dataframe to the list\n",
    "        dataframe_list.append(prediction_metrics_df)\n",
    "\n",
    "    ## Connect the results from the 3 binary classification tasks;\n",
    "    ArrangedData=pd.concat(dataframe_list, axis=1, join=\"outer\") \n",
    "    ArrangedData[\"multiclass_accuracy\"]=multiclass_accuracy\n",
    "    \n",
    "    ## save the results to excel\n",
    "    ArrangedData.to_excel(save_prediction_metrics_excel)\n",
    "    display(ArrangedData.head())\n",
    "\n",
    "    \n",
    "def arrange_CV_results(binary_task_path_dict, save_results_basepath):\n",
    "    \"\"\"\n",
    "    Function to arrange the cross-validation AUC results by different classifiers.\n",
    "    \"\"\"\n",
    "    \n",
    "    save_CV_results_excel=os.path.join(save_results_basepath, \"CV_AUC_results.xlsx\")\n",
    "    \n",
    "    ## read the results from the three binary classification tasks into a data frame list, to preprare for data frame connection.\n",
    "    dataframe_list=[]\n",
    "    for binary_task_name, binary_task_path in binary_task_path_dict.items():\n",
    "        best_model_name=traversalDir_FirstDir(binary_task_path)\n",
    "        assert len(best_model_name)==1\n",
    "        \n",
    "        # read the cross-validation results;\n",
    "        CV_AUC_results_file=os.path.join(binary_task_path, \"AUC_results_all_models.txt\")\n",
    "        CV_AUC_results_df=pd.read_csv(CV_AUC_results_file, index_col=0)\n",
    "        CV_AUC_results_df.insert(0, \"binary_task_name\", binary_task_name)\n",
    "     \n",
    "        # append the dataframe to the list\n",
    "        dataframe_list.append(CV_AUC_results_df)\n",
    "\n",
    "    ## Connect the results from the 3 binary classification tasks;\n",
    "    ArrangedData=pd.concat(dataframe_list, axis=0, join=\"outer\") \n",
    "    \n",
    "    ## save the results to excel\n",
    "    ArrangedData.to_excel(save_CV_results_excel)\n",
    "    display(ArrangedData.head())\n",
    "    \n",
    "    \n",
    "def main_convert_binary_to_multiclass(binary_task_path_dict, save_results_basepath, ground_truth_target_excel_dict, data_folder):\n",
    "    \"\"\"\n",
    "    Main function to perform the convertion: convert from the 3 binary classification problem to 1 multi-class classification problem.\n",
    "    \"\"\"\n",
    "    threshold=0.5\n",
    "    \n",
    "    print(\"\\n\\n==============  {}   =================\".format(data_folder))\n",
    "    save_multiclass_results_excel=os.path.join(save_results_basepath, \"multiclass_predicted_results-\"+data_folder+\".xlsx\")\n",
    "    \n",
    "        \n",
    "    ## read the ground truth data to get the ground truth label.\n",
    "    GT_data=pd.read_excel(ground_truth_target_excel_dict[data_folder], index_col=0)\n",
    "    GT_subtypes=GT_data.loc[:, [\"tumor_subtype_description\", \"tumor_subtype\"]]\n",
    "    \n",
    "    ## read the results from the three binary classification tasks into a data frame list, to preprare for data frame connection.\n",
    "    dataframe_list=[]\n",
    "    for binary_task_name, binary_task_path in binary_task_path_dict.items():\n",
    "        best_model_name=traversalDir_FirstDir(binary_task_path)\n",
    "        assert len(best_model_name)==1\n",
    "\n",
    "        # read the predicted probability\n",
    "        predicted_prob_file=os.path.join(binary_task_path, best_model_name[0], data_folder, \"predicted_prob.csv\")\n",
    "        predicted_prob_df=pd.read_csv(predicted_prob_file, index_col=0)\n",
    "        \n",
    "        # read the predicted results;\n",
    "        predicted_results_file=os.path.join(binary_task_path, best_model_name[0], data_folder, \"predicted.csv\")\n",
    "        predicted_df=pd.read_csv(predicted_results_file, index_col=0)\n",
    "        \n",
    "        # connect the predicted probability and the predicted results.\n",
    "        data_df=pd.concat([predicted_prob_df, predicted_df], axis=1, join=\"outer\")\n",
    "        data_df[binary_task_name]=GT_data[binary_task_name]\n",
    "        \n",
    "        # rename the columns\n",
    "        data_df[binary_task_name+\"-best_model_name\"]=best_model_name[0]\n",
    "        data_df.rename(columns={\"predicted_prob\": binary_task_name+\"-predicted_prob\",\n",
    "                                \"predicted\": binary_task_name+\"-predicted\",\n",
    "                                binary_task_name: binary_task_name+\"-true\"}, inplace=True)\n",
    "        \n",
    "        # append the dataframe to the list\n",
    "        dataframe_list.append(data_df)\n",
    "\n",
    "    ## Connect the results from the 3 binary classification tasks;\n",
    "    ArrangedData=pd.concat(dataframe_list, axis=1, join=\"outer\") \n",
    "    \n",
    "    ## convert from 3 binary classification results to 1 multiclass results;\n",
    "    ArrangedData[\"predicted_tumor_subtype_description\"]=ArrangedData.apply(get_tumor_subtype_description, axis=1)\n",
    "    ArrangedData[\"predicted_tumor_subtype\"]=ArrangedData.apply(get_tumor_subtype, axis=1)\n",
    "    \n",
    "    ## Connet the predicted multiclass label with the ground truth label.\n",
    "    ArrangedData=pd.concat([ArrangedData, GT_subtypes], axis=1, join=\"outer\") \n",
    "    \n",
    "    ## save the results to excel\n",
    "    ArrangedData.to_excel(save_multiclass_results_excel)\n",
    "    display(ArrangedData.head())\n",
    "\n",
    "    ## calculate the accuracy\n",
    "    y_true=ArrangedData[\"tumor_subtype\"]\n",
    "    y_predicted=ArrangedData[\"predicted_tumor_subtype\"]\n",
    "    result_metrics=calculate_metrics(y_true, y_predicted)\n",
    "    classification_reports=classification_report(y_true, y_predicted, output_dict=True)\n",
    "    classification_reports=pd.DataFrame(classification_reports)\n",
    "    print(\"\\n\\n Multiclass classification metrics: \\n {}.\".format(result_metrics))\n",
    "    print(\"\\n\\n classification reports:\\n{}.\".format(classification_reports))\n",
    "    \n",
    "    ## save the metrics\n",
    "    save_dict(result_metrics, os.path.join(save_results_basepath, \"metrics-\"+data_folder+\".txt\"))\n",
    "    classification_reports.to_csv(os.path.join(save_results_basepath, \"classification_reports-\"+data_folder+\".txt\"))\n",
    "    \n",
    "    ## plot confusion matrix\n",
    "    save_confusion_matrix_path=os.path.join(save_results_basepath, \"confusion_matrix-\"+data_folder+\".jpeg\")\n",
    "    plot_confusion_matrix(y_true, y_predicted, save_confusion_matrix_path)\n",
    "    \n",
    "    # arrange the prediction metrics and the cross-validation AUC results.\n",
    "    arrange_prediction_metrics(binary_task_path_dict, save_results_basepath, data_folder, result_metrics[\"accuracy\"])\n",
    "    arrange_CV_results(binary_task_path_dict, save_results_basepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_seed_list=get_random_seed_list()\n",
    "num_random_seeds=len(random_seed_list)\n",
    "for i, random_seed in enumerate(random_seed_list):\n",
    "    print(\"\\n\\n ====================== {}/{}: random_seed={}===================\".format(i+1, num_random_seeds, random_seed))\n",
    "    # read the settings\n",
    "    convert_binary_to_multiclass_setting_dict=get_convert_binary_to_multiclass_setting_dict(random_seed)\n",
    "\n",
    "    # perform convertion for each setting.\n",
    "    for setting_name, convert_binary_to_multiclass_setting in convert_binary_to_multiclass_setting_dict.items():\n",
    "\n",
    "        binary_task_path_dict=convert_binary_to_multiclass_setting[\"binary_task_path_dict\"]\n",
    "        save_results_basepath=convert_binary_to_multiclass_setting[\"save_results_basepath\"]\n",
    "        ground_truth_target_excel_dict=convert_binary_to_multiclass_setting[\"ground_truth_target_excel_dict\"]\n",
    "\n",
    "        # convertion for train data.\n",
    "        main_convert_binary_to_multiclass(binary_task_path_dict, save_results_basepath, \n",
    "                                          ground_truth_target_excel_dict, data_folder=\"train_data\")\n",
    "        # convertion for test data.\n",
    "        main_convert_binary_to_multiclass(binary_task_path_dict, save_results_basepath, \n",
    "                                          ground_truth_target_excel_dict, data_folder=\"test_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main: Calculate the mean/std deviation of the cross-validation AUCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed_list=get_random_seed_list()\n",
    "\n",
    "dataframe_list=[]\n",
    "for i, random_seed in enumerate(random_seed_list):\n",
    "    # read the settings\n",
    "    convert_binary_to_multiclass_setting_dict=get_convert_binary_to_multiclass_setting_dict(random_seed)\n",
    "\n",
    "    # perform convertion for each setting.\n",
    "    for setting_name, convert_binary_to_multiclass_setting in convert_binary_to_multiclass_setting_dict.items():\n",
    "\n",
    "        save_results_basepath=convert_binary_to_multiclass_setting[\"save_results_basepath\"]\n",
    "        CV_AUC_results_excel=os.path.join(save_results_basepath, \"CV_AUC_results.xlsx\")\n",
    "        CV_AUC_results_df=pd.read_excel(CV_AUC_results_excel, index_col=0)\n",
    "        CV_AUC_results_df.insert(0, \"random_seed\", random_seed)\n",
    "        CV_AUC_results_df.insert(0, \"setting_name\", setting_name)\n",
    "        \n",
    "        dataframe_list.append(CV_AUC_results_df)\n",
    "        \n",
    "## connect the prediction metrics of different random seeds.\n",
    "ArrangedResults=pd.concat(dataframe_list, axis=0, join=\"outer\") \n",
    "ArrangedResults.reset_index(drop=True, inplace=True)\n",
    "ArrangedResults.to_excel(os.path.join(os.path.dirname(save_results_basepath), \"all_CV_AUC_results.xlsx\"))\n",
    "display(ArrangedResults)\n",
    "\n",
    "## statistic results\n",
    "ArrangedResults.drop(columns=[\"setting_name\", \"random_seed\", \"Time(seconds)\"], inplace=True)\n",
    "statistic_CV_results=ArrangedResults.groupby([\"binary_task_name\", \"model_name\"]).describe()\n",
    "statistic_CV_results.to_excel(os.path.join(os.path.dirname(save_results_basepath), \"statistics_CV_AUC_results.xlsx\"))\n",
    "\n",
    "display(statistic_CV_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main: Calculate the mean/std deviation of the prediction metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "random_seed_list=get_random_seed_list()\n",
    "num_random_seeds=len(random_seed_list)\n",
    "\n",
    "data_folder=\"test_data\" #\"train_data\"\n",
    "\n",
    "dataframe_list=[]\n",
    "for i, random_seed in enumerate(random_seed_list):\n",
    "    # read the settings\n",
    "    convert_binary_to_multiclass_setting_dict=get_convert_binary_to_multiclass_setting_dict(random_seed)\n",
    "\n",
    "    # perform convertion for each setting.\n",
    "    for setting_name, convert_binary_to_multiclass_setting in convert_binary_to_multiclass_setting_dict.items():\n",
    "\n",
    "        save_results_basepath=convert_binary_to_multiclass_setting[\"save_results_basepath\"]\n",
    "        prediction_metrics_excel=os.path.join(save_results_basepath, \"prediction_metrics-\"+data_folder+\".xlsx\")\n",
    "        \n",
    "        prediction_metrics_df=pd.read_excel(prediction_metrics_excel, index_col=0)\n",
    "        prediction_metrics_df[\"setting_name\"]=setting_name\n",
    "        prediction_metrics_df[\"data_folder\"]=data_folder\n",
    "        prediction_metrics_df[\"random_seed\"]=random_seed\n",
    "        prediction_metrics_df.set_index(\"random_seed\", inplace=True)\n",
    "        \n",
    "        dataframe_list.append(prediction_metrics_df)\n",
    "        \n",
    "## connect the prediction metrics of different random seeds.\n",
    "ArrangedResults=pd.concat(dataframe_list, axis=0, join=\"outer\") \n",
    "ArrangedResults.to_excel(os.path.join(os.path.dirname(save_results_basepath), \"all_prediction_metrics-\"+data_folder+\".xlsx\"))\n",
    "display(ArrangedResults)\n",
    "\n",
    "## Caclulate the mean/std/medial\n",
    "mean=ArrangedResults.mean(axis=0)\n",
    "std=ArrangedResults.std(axis=0)\n",
    "var=ArrangedResults.var(axis=0)\n",
    "median=ArrangedResults.median(axis=0)\n",
    "mad=ArrangedResults.mad(axis=0)\n",
    "\n",
    "statistic_results_dict={\"mean\":mean, \"std\": std, \"var\": var, \"median\":median, \"mad\":mad} \n",
    "statistic_results_df=pd.DataFrame(statistic_results_dict)\n",
    "statistic_results_df.to_excel(os.path.join(os.path.dirname(save_results_basepath), \"statistics_of_prediction_metrics-\"+data_folder+\".xlsx\"))\n",
    "\n",
    "display(statistic_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
