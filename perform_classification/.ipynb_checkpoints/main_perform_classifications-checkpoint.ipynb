{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform feature selection and classification to predict the gene status of the gliomas patients.\n",
    "\n",
    "For example, we can use this code and the radiomic features to predict:\n",
    "- 1. LGG vs. GBM;\n",
    "- 2. IDH mutant vs. IDH wildtype;\n",
    "- 3. 1p/19q codeleted vs. 1p/19q intact;\n",
    "- 4. MGMT methylated vs. MGMT unmethylated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "import operator\n",
    "import joblib\n",
    "\n",
    "## For plots\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## For preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# For feature selection and classifier models\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, LassoCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectKBest, SelectFromModel, RFECV, RFE, f_classif\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import svm, feature_selection\n",
    "\n",
    "# For evaluation metrics\n",
    "from sklearn import metrics\n",
    "\n",
    "## self-defined functions\n",
    "from mySettings import *\n",
    "from harmonizationUtils import neuroComBat_harmonization\n",
    "\n",
    "## feature selection\n",
    "from probatus.feature_elimination import ShapRFECV\n",
    "\n",
    "#import function from the self-defined utils\n",
    "import sys\n",
    "sys.path.append(\"E://2020_MRI_Work/HarmonizationProject\")\n",
    "from utils.myUtils import mkdir, save_dict, load_dict, get_logger, save_pickle, load_pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seed\n",
    "random_seed=2021\n",
    "\n",
    "#public \"logger\" to save the logs.\n",
    "_, task_settings=get_classification_task_settings()\n",
    "base_results_path=task_settings[\"base_results_path\"]\n",
    "log_file_path=os.path.join(base_results_path, \"log.txt\")\n",
    "logger=get_logger(log_file_path)\n",
    "    \n",
    "# save log function.\n",
    "def save_log(string):\n",
    "    #print(string)\n",
    "    logger.info(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: find the best model.\n",
    "\n",
    "- from a list of models, with different feature selection methods and classifiers;\n",
    "- using the train data and 5-folds cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning_for_different_models(X, y, save_results_path, feature_selection_type):\n",
    "    \"\"\"\n",
    "    Tuning the hypperparameters for different models.\n",
    "    \"\"\"\n",
    "    ##====== classifiers =======\n",
    "    classification_models=dict()\n",
    "    classification_models[\"SVM\"]=svm.SVC()\n",
    "    classification_models[\"Perceptron\"]=Perceptron()\n",
    "    classification_models[\"LogisticRegression\"]=LogisticRegression()\n",
    "    classification_models[\"RandomForest\"]=RandomForestClassifier()     \n",
    "    classification_models[\"DecisionTree\"]=DecisionTreeClassifier()\n",
    "    classification_models[\"ExtraTrees\"]=ExtraTreesClassifier() \n",
    "    #classification_models[\"LightGBM\"]=LGBMClassifier()\n",
    "    classification_models[\"XGBClassifier\"]=XGBClassifier()\n",
    "    classification_models[\"GradientBoosting\"]=GradientBoostingClassifier()\n",
    "    \n",
    "\n",
    "    ##======  hyperparameters  =======\n",
    "    param_grids=dict()\n",
    "    param_grids[\"SVM\"]=[{\n",
    "        \"kernel\":[\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "        \"C\":[0.5, 1, 1.5, 2],\n",
    "        \"gamma\":[\"scale\"],\n",
    "        \"class_weight\":[\"balanced\"],\n",
    "        \"random_state\":[random_seed]\n",
    "    }]\n",
    "    \n",
    "    param_grids[\"Perceptron\"]=[{\n",
    "        \"penalty\": [\"l1\", \"l2\", \"elasticnet\", None],\n",
    "        \"alpha\":[0.001, 0.0001, 0.00001],\n",
    "        \"class_weight\":[\"balanced\"],\n",
    "        \"random_state\":[random_seed]\n",
    "    }]\n",
    "    \n",
    "    param_grids[\"LogisticRegression\"]=[\n",
    "        ## l1 penalty\n",
    "        {\"penalty\": [\"l1\"],\n",
    "        \"C\":[0.5, 1, 1.5, 2],\n",
    "        \"solver\": [\"liblinear\", \"saga\"],\n",
    "        \"class_weight\":[\"balanced\"],\n",
    "        #\"max_iter\": [500],\n",
    "        \"random_state\":[random_seed]},\n",
    "        ## l2 penalty\n",
    "        {\"penalty\": [\"l2\"], #, 'none'\n",
    "        \"C\":[0.5, 1, 1.5, 2],\n",
    "        \"solver\": [\"newton-cg\", \"lbfgs\", \"sag\", \"saga\"],\n",
    "        \"class_weight\":[\"balanced\"],\n",
    "        #\"max_iter\": [500],\n",
    "        \"random_state\":[random_seed]} \n",
    "    ]\n",
    "    \n",
    "        \n",
    "    param_grids[\"RandomForest\"]=[{\n",
    "        'n_estimators':  [10, 20, 40, 60, 80, 100],\n",
    "        'max_features': ['auto', 'sqrt'],\n",
    "        'max_depth':   [5, 10, 20, 30, 40, 50, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 5, 10],\n",
    "        'bootstrap': [True, False],\n",
    "        \"random_state\":[random_seed]\n",
    "    }]\n",
    "    \n",
    "    param_grids[\"DecisionTree\"]=[{\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        \"max_depth\":  [5, 10, 20, 30, 40, 50, None],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 5, 10],\n",
    "        \"random_state\":[random_seed],\n",
    "        \"class_weight\":[\"balanced\"],\n",
    "        \"random_state\":[random_seed]\n",
    "    }]\n",
    "        \n",
    "        \n",
    "    param_grids[\"ExtraTrees\"]=[{\n",
    "        \"n_estimators\":[10, 20, 40, 60, 80, 100],\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        'max_depth':  [10, 20, 30, 40, 50, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 5, 10],\n",
    "        'max_features': ['auto', 'sqrt', \"log2\"],\n",
    "        \"class_weight\":[\"balanced\", \"balanced_subsample\"],\n",
    "        \"random_state\":[random_seed]\n",
    "    }]\n",
    "        \n",
    "    \n",
    "    param_grids[\"LightGBM\"]=[{\n",
    "        \"application\": [\"binary\"],\n",
    "        \"boosting\": [\"gbdt\", \"rf\", \"dart\", \"goss\"], \n",
    "        #\"num_boost_round\": [50, 100, 200], \n",
    "        \"learning_rate\": [0.001, 0.01, 0.1],\n",
    "        \"num_leaves\": [21, 31, 51], \n",
    "        \"device\": [\"gpu\"],\n",
    "        \"max_depth\":  [10, 20, 30, 40, 50, None],\n",
    "        \"min_data_in_leaf\":  [1, 2, 5, 10, 20],\n",
    "        \"reg_lambda\": [0.001, 0.01, 0.1, 0.2, 0.3],\n",
    "        \"verbose\": [-1],\n",
    "        \"random_state\":[random_seed]\n",
    "    }]\n",
    "        \n",
    "    param_grids[\"XGBClassifier\"]=[{\n",
    "        \"n_estimators\":  [10, 20, 40, 60, 80, 100],\n",
    "        'max_depth':  [10, 20, 30, 40, 50, None],\n",
    "        \"learning_rate\": [0.001, 0.01, 0.1],\n",
    "        \"booster\": [\"gbtree\", \"gblinear\", \"dart\"],\n",
    "        #'min_child_weight': [1, 5, 10],\n",
    "        #'gamma': [0.5, 1, 2, 5],\n",
    "        'subsample':  [0.3, 0.7, 1], \n",
    "        #'colsample_bytree':  [0, 0.3, 0.7, 1], \n",
    "        #'colsample_bylevel':  [0, 0.3, 0.7, 1], \n",
    "        'reg_alpha': [0, 1],\n",
    "        'reg_lambda': [0, 1],\n",
    "        \"use_label_encoder\": [False],\n",
    "        \"eval_metric\": [\"logloss\"], \n",
    "        \"random_state\":[random_seed]\n",
    "    }]\n",
    "    \n",
    "    \n",
    "    param_grids[\"GradientBoosting\"]=[{\n",
    "        \"n_estimators\": [10, 20, 40, 60, 80, 100],\n",
    "        'max_depth':  [10, 20, 30, 40, 50, None],\n",
    "        \"learning_rate\": [0.001, 0.01, 0.1],\n",
    "        \"loss\": [\"deviance\", \"exponential\"],\n",
    "        \"subsample\":  [0.3, 0.7, 1], \n",
    "        \"criterion\": [\"friedman_mse\", \"mse\"],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 5, 10],\n",
    "        \"random_state\":[random_seed]\n",
    "    }]\n",
    "    \n",
    "    ## feature numbers for random search\n",
    "    feature_number_for_selection=[10, 30, 50, 100]\n",
    "    \n",
    "    ## ============ Models =================\n",
    "    for classfier_name, classifier_model in classification_models.items():\n",
    "        save_log(\"\\n\\n ======Exploring the hyperparameters for feature_selection_method={}, classifier={}. =========\".format(feature_selection_type, classfier_name))\n",
    "        start_time = time()\n",
    "        \n",
    "        ### Scaler\n",
    "        Scaler=StandardScaler()  # MinMaxScaler(feature_range=(0,1))\n",
    "        \n",
    "        #--------------------- begin hyperparameter tuning process--------------------------------\n",
    "        ### define feature selection function.\n",
    "        if feature_selection_type==\"RFE\":\n",
    "            feature_selection_method=RFE(estimator=classifier_model, step=5) #, n_features_to_select=20\n",
    "            pipeline = Pipeline(steps=[('scaler', Scaler), \n",
    "                                       ('feature_selection',feature_selection_method)])\n",
    "            #save_log(\"Possible hyperparameters for {} pipeline: \\n {}\".format(classfier_name, pipeline.get_params().keys()))\n",
    "            \n",
    "            param_grid_feature_selection_list=[{\"feature_selection__estimator__\"+key: item for key, item in param_grid_dict.items()} for param_grid_dict in param_grids[classfier_name]]\n",
    "            randomsearch_param_grids=[dict(**{\"feature_selection__n_features_to_select\": feature_number_for_selection}, **param_grid_feature_selection) for param_grid_feature_selection in param_grid_feature_selection_list]\n",
    "            \n",
    "            search = RandomizedSearchCV(pipeline, randomsearch_param_grids, cv=5, n_iter=50, scoring=\"roc_auc\", random_state=random_seed, verbose=2).fit(X, y)\n",
    "            n_feature_selected=search.best_estimator_[\"feature_selection\"].n_features_\n",
    "            \n",
    "        elif feature_selection_type==\"RFECV\": \n",
    "            feature_selection_method=RFECV(estimator=classifier_model, step=5, min_features_to_select=20) \n",
    "            pipeline = Pipeline(steps=[('scaler', Scaler), \n",
    "                                       ('feature_selection',feature_selection_method)])\n",
    "            \n",
    "            randomsearch_param_grids=[{\"feature_selection__estimator__\"+key: item for key, item in param_grid_dict.items()} for param_grid_dict in param_grids[classfier_name]]\n",
    "\n",
    "            search = RandomizedSearchCV(pipeline, randomsearch_param_grids, cv=5, n_iter=50, scoring=\"roc_auc\", random_state=random_seed, verbose=2).fit(X, y)\n",
    "            n_feature_selected=search.best_estimator_[\"feature_selection\"].n_features_\n",
    "            \n",
    "        elif feature_selection_type==\"SelectFromModel\": \n",
    "            feature_selection_method=SelectFromModel(estimator=classifier_model) #max_features=20\n",
    "            pipeline = Pipeline(steps=[('scaler', Scaler), \n",
    "                                       ('feature_selection',feature_selection_method)])\n",
    "            \n",
    "            param_grid_feature_selection_list=[{\"feature_selection__estimator__\"+key: item for key, item in param_grid_dict.items()} for param_grid_dict in param_grids[classfier_name]]\n",
    "            randomsearch_param_grids=[dict(**{\"feature_selection__max_features\": feature_number_for_selection}, **param_grid_feature_selection) for param_grid_feature_selection in param_grid_feature_selection_list]\n",
    "            \n",
    "            search = RandomizedSearchCV(pipeline, randomsearch_param_grids, cv=5, n_iter=50, scoring=\"roc_auc\", random_state=random_seed, verbose=2).fit(X, y)\n",
    "            n_feature_selected= search.best_estimator_.transform(X).shape[1]\n",
    "            \n",
    "        elif feature_selection_type==\"AnovaTest\": \n",
    "            feature_selection_method=SelectKBest(score_func=f_classif) # k=n_features_to_select\n",
    "            pipeline = Pipeline(steps=[('scaler', Scaler),  \n",
    "                                       ('feature_selection',feature_selection_method),\n",
    "                                       ('classifier',classifier_model)])\n",
    "            \n",
    "            param_grid_classifier_list=[{\"classifier__\"+key: item for key, item in param_grid_dict.items()} for param_grid_dict in param_grids[classfier_name]]\n",
    "            randomsearch_param_grids=[dict(**{\"feature_selection__k\": feature_number_for_selection}, **param_grid_classifier) for param_grid_classifier in param_grid_classifier_list]\n",
    "            \n",
    "            search = RandomizedSearchCV(pipeline, randomsearch_param_grids, cv=5, n_iter=50, scoring=\"roc_auc\", random_state=random_seed, verbose=1).fit(X, y)\n",
    "            n_feature_selected=search.best_estimator_[\"feature_selection\"].k\n",
    "            \n",
    "        else:\n",
    "            raise Exception(\"Undefined feature selection function: {} !!\".format(feature_selection_type))\n",
    "        \n",
    "        \n",
    "        #--------------------- end hyperparameter tuning process--------------------------------\n",
    "        \n",
    "        ### get the best estimator.\n",
    "        if feature_selection_type==\"SelectFromModel\":\n",
    "            best_estimator=Pipeline(steps=[('scaler', search.best_estimator_['scaler']), \n",
    "                                   ('feature_selection',search.best_estimator_['feature_selection']),\n",
    "                                   ('classifier',search.best_estimator_['feature_selection'].estimator_)])\n",
    "        else:\n",
    "            best_estimator= search.best_estimator_\n",
    "         \n",
    "        \n",
    "        ### arrange the results and save it into a dict.\n",
    "        save_classifier_name=feature_selection_type+\"_\"+classfier_name\n",
    "        result={'classfier_name':save_classifier_name,\n",
    "                'best score': search.best_score_, \n",
    "                'best params': search.best_params_,\n",
    "                'time_cost':time()-start_time,\n",
    "                'n_feature_selected': n_feature_selected,\n",
    "                #'grid': search, \n",
    "                'best_estimator': best_estimator,\n",
    "                #'cv': search.cv,\n",
    "                'scorer':search.scorer_,\n",
    "                'cv_results_': pd.DataFrame(search.cv_results_) \n",
    "                }\n",
    " \n",
    "        ### save the results\n",
    "        save_txt_path=os.path.join(save_results_path, \"RandomizedSearchCV_\"+save_classifier_name+\".pickle\")\n",
    "        save_pickle(result, save_txt_path)\n",
    "        save_log(\"Best parameter for {}: \\n result={}.\".format(save_classifier_name, result))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_classifier_list():\n",
    "    \"\"\"\n",
    "    List of the models considered for comparison.\n",
    "    \"\"\"\n",
    "    \n",
    "    feature_selection_method_list=[\"RFE\", \"RFECV\", \"AnovaTest\", \"SelectFromModel\"]\n",
    "    classifier_list=[\"SVM\", \"Perceptron\", \"LogisticRegression\", \"RandomForest\", \"DecisionTree\",\n",
    "                     \"ExtraTrees\", \"LightGBM\", \"GradientBoosting\", \"XGBClassifier\"]\n",
    "    \n",
    "    classifiers=[]\n",
    "    for feature_selection_type in feature_selection_method_list:\n",
    "        for classfier_name in classifier_list:\n",
    "            classifiers.append(feature_selection_type+\"_\"+classfier_name)\n",
    "            \n",
    "    return classifiers\n",
    "\n",
    "\n",
    "\n",
    "def arrange_hyperparameter_searching_results(results_path):   \n",
    "    \"\"\"\n",
    "    Arrange the results of all different classifiers, after performing best hyperparameter searching for each classifier.\n",
    "    \"\"\"\n",
    "    \n",
    "    classifiers=get_all_classifier_list()\n",
    "    \n",
    "    Results=[]\n",
    "    for classfier_name in classifiers:\n",
    "        hyperparameter_result_path=os.path.join(results_path, \"RandomizedSearchCV_\"+classfier_name+\".pickle\")\n",
    "        if os.path.exists(hyperparameter_result_path):\n",
    "            hyperparameter_result=load_pickle(hyperparameter_result_path)\n",
    "            Results.append(hyperparameter_result)\n",
    "        \n",
    "    ## Sorting results by best score\n",
    "    Results = sorted(Results, key=operator.itemgetter('best score'), reverse=True)\n",
    "    save_pickle(Results, os.path.join(results_path, \"RandomizedSearchCV_all_models.pickle\"))\n",
    "    save_log(\"\\n\\n ******RandomizedSearchCV Results: *****\\n{}\".format(Results))\n",
    "    \n",
    "    ## best classifier\n",
    "    best_classifier_name=Results[0][\"classfier_name\"]\n",
    "    \n",
    "    return best_classifier_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y):\n",
    "    \"\"\"\n",
    "    Function for evaluating the model.\n",
    "    \"\"\"\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=random_seed)\n",
    "    scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1, error_score='raise') \n",
    "    \n",
    "    return scores\n",
    "\n",
    "def get_different_models_from_pickle(model_basepath):\n",
    "    \"\"\"\n",
    "    Define the models from the pickle files which saves the best hyperparameters trainded by RandomSearchCV.\n",
    "    \"\"\"\n",
    "    \n",
    "    save_log(\"\\n Load the model paramters from {}.\".format(model_basepath))\n",
    "    \n",
    "    ## read the models from the pickle files.\n",
    "    models=dict()\n",
    "    classifiers=get_all_classifier_list()\n",
    "    for classfier_name in classifiers:\n",
    "        hyperparameter_result_path=os.path.join(model_basepath, \"RandomizedSearchCV_\"+classfier_name+\".pickle\")\n",
    "        if os.path.exists(hyperparameter_result_path):\n",
    "            hyperparameter_result=load_pickle(hyperparameter_result_path)\n",
    "            best_estimator=hyperparameter_result[\"best_estimator\"]\n",
    "            models[classfier_name]=best_estimator\n",
    "            save_log(\"\\n Use the best hyperparameter found by RandomSearchCV  for {}: {}.\".format(classfier_name, best_estimator))\n",
    "\n",
    "    return models\n",
    "\n",
    "\n",
    "\n",
    "def explore_different_models(X, y, save_results_path):\n",
    "    \"\"\"\n",
    "    Exploring the models with different feature selection and classifiers, and show the accuracy of these models.\n",
    "    \"\"\"\n",
    "\n",
    "    # get the models to evaluate\n",
    "    models = get_different_models_from_pickle(save_results_path)\n",
    "    \n",
    "    # evaluate the models and save the results\n",
    "    results=[]\n",
    "    for model_name, model in models.items():\n",
    "        start_time = time()\n",
    "        scores = evaluate_model(model, X, y)\n",
    "        time_cost=time()-start_time\n",
    "        results.append((model_name, np.median(scores), np.mean(scores), np.std(scores), time_cost, scores))\n",
    "        save_log('> %s: median_score= %.3f , mean_score= %.3f , std_score= %.3f, time=%.2f seconds.' % (model_name, np.median(scores), np.mean(scores), np.std(scores), time_cost))\n",
    "    \n",
    "    results=pd.DataFrame(results, columns=[\"model_name\", \"median_AUC\", \"mean_AUC\", \"std_AUC\", \"Time(seconds)\", \"AUC\"])\n",
    "    results.sort_values(\"mean_AUC\", ascending=False, inplace=True)\n",
    "    results.to_csv(os.path.join(save_results_path, \"AUC_results_all_models.txt\"))\n",
    "    save_log(\"\\n\\n ***********rank average of the AUC scores: ************\\n{}\".format(results))\n",
    "    \n",
    "    # plot model performance for comparison\n",
    "    plt.subplots(figsize=(15,5))\n",
    "    plt.boxplot(results[\"AUC\"], labels=results[\"model_name\"], showmeans=True)\n",
    "    plt.xlabel('Feature selection and classifier models', fontsize=15)\n",
    "    plt.ylabel('AUC',fontsize=15)\n",
    "    plt.xticks(rotation=15)\n",
    "    plt.subplots_adjust(left=0.05, bottom=0.25, right=0.95, top=0.95, wspace =0, hspace =0) \n",
    "    save_fig_path=os.path.join(save_results_path, \"explore_different_models.jpeg\")\n",
    "    plt.savefig(save_fig_path)\n",
    "    plt.show()\n",
    "    \n",
    "    # best classifier\n",
    "    best_classifier_name=results.iloc[0][\"model_name\"]\n",
    "\n",
    "    return best_classifier_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_find_best_model(train_X, train_Y, save_results_path, feature_selection_type):\n",
    "    \"\"\"\n",
    "    Step 1: Tuning the hyperparameters for different feature selection and classifier models.\n",
    "    \"\"\"\n",
    "    save_log(\"\\n\\n == Tuning the hyperparameters for different feature selection and classifier models... ==\")\n",
    "    hyperparameter_tuning_for_different_models(train_X, train_Y, save_results_path, feature_selection_type)\n",
    "    arrange_hyperparameter_searching_results(save_results_path)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Step 2: Compare the results of different feature selection and classifier models, with the best tuned hyperparameters.\n",
    "    \"\"\"\n",
    "    save_log(\"\\n\\n == Compare the results of different feature selection and classifier models, with the best tuned hyperparameters... ==\")\n",
    "    best_model_name= explore_different_models(train_X, train_Y, save_results_path)\n",
    "    \n",
    "    return best_model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: use the best model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_the_best_model(train_X, train_Y, best_model_name, save_results_path):\n",
    "    \"\"\"\n",
    "    Retrain the best model with the whole training dataset. \n",
    "    \"\"\"\n",
    "    \n",
    "    save_log(\"\\nWe Retrain the model {} using the whole training dataset. \".format(best_model_name))\n",
    "    \n",
    "    # fit the model on training data.\n",
    "    models=get_different_models_from_pickle(save_results_path)\n",
    "    best_model=models[best_model_name]\n",
    "    best_model.fit(train_X, train_Y)\n",
    "    \n",
    "    # save the trained model.\n",
    "    save_results_path=os.path.join(save_results_path, best_model_name)\n",
    "    if not os.path.exists(save_results_path):\n",
    "        os.makedirs(save_results_path)\n",
    "        \n",
    "    save_trained_model_path=os.path.join(save_results_path, 'trained_model.sav')\n",
    "    joblib.dump(best_model, save_trained_model_path)\n",
    "    \n",
    "    return save_trained_model_path\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: prediction with the trained best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ROC_curve(y_true, predicted_prob, save_results_path):\n",
    "    \"\"\"\n",
    "    Plot the ROC curve.\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate the fpr/tpr values and AUC\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_true, predicted_prob)\n",
    "    save_log(\"thresholds={}\".format(thresholds))\n",
    "    roc_auc_score = metrics.auc(fpr, tpr)  \n",
    "    \n",
    "    #plot the ROC curve\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 6))\n",
    "    ax.plot(fpr, tpr, color='darkorange', lw=3, label='area = %0.2f' % roc_auc_score)\n",
    "    ax.plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "    ax.set(xlabel='False Positive Rate', ylabel=\"True Positive Rate (Recall)\", title=\"Receiver Operating Characteristic\")     \n",
    "    ax.legend(loc=\"lower right\")\n",
    "    ax.grid(True)\n",
    "    plt.savefig(os.path.join(save_results_path, \"ROC_curve.jpeg\"))         \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def select_threshold(y_true, predicted_prob, save_results_path):\n",
    "    \n",
    "    #Calculate the evaluation metrics.\n",
    "    metrics_dict={\"accuracy\":[], \"precision\":[], \"recall\":[], \"F1\":[]}\n",
    "    thresholds=np.arange(0.1, 1, step=0.1)\n",
    "    for threshold in thresholds:\n",
    "        predicted= predicted_prob>threshold\n",
    "        \n",
    "        metrics_dict[\"accuracy\"].append(metrics.accuracy_score(y_true, predicted))\n",
    "        metrics_dict[\"recall\"].append(metrics.recall_score(y_true, predicted))\n",
    "        metrics_dict[\"precision\"].append(metrics.precision_score(y_true, predicted))\n",
    "        metrics_dict[\"F1\"].append(metrics.f1_score(y_true, predicted))\n",
    "    \n",
    "    metrics_df=pd.DataFrame(metrics_dict).set_index(pd.Index(thresholds))\n",
    "    \n",
    "    # plot the values of these metrics depending on different threshold\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 4))\n",
    "    metrics_df.plot(ax=ax)\n",
    "    ax.set(xlabel='threshold', ylabel=\"metrics\", title=\"Threshold Selection\")\n",
    "    ax.legend(loc=\"lower left\")\n",
    "    ax.grid(True)\n",
    "    plt.savefig(os.path.join(save_results_path, \"threshold_selection.jpeg\"))  \n",
    "    plt.show()\n",
    "    \n",
    "    # Choose the threshold which maximize the F1-score.\n",
    "    best_threshold=metrics_dict[\"F1\"].idxmax()\n",
    "    \n",
    "    return best_threshold\n",
    " \n",
    "    \n",
    "def plot_confusion_matrix(y_true, predicted, save_results_path):\n",
    "    \"\"\"\n",
    "    Plot the confusion matrix.\n",
    "    \"\"\"\n",
    "    classes = np.unique(y_true)\n",
    "    cm = metrics.confusion_matrix(y_true, predicted, labels=classes)\n",
    "    \n",
    "    # plot the confusion matrix.\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=plt.cm.Blues, cbar=False)\n",
    "    ax.set(xlabel=\"Pred\", ylabel=\"True\", title=\"Confusion matrix\")\n",
    "    ax.set_yticklabels(labels=classes)\n",
    "    plt.savefig(os.path.join(save_results_path, \"confusion_matrix.jpeg\"))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def calculate_metrics(y_true, predicted, predicted_prob):\n",
    "    \"\"\"\n",
    "    Calcualte the metrics for evaluation.\n",
    "    \"\"\"\n",
    "    result_metrics={}\n",
    "    # metrics based on the predicted_prob.\n",
    "    result_metrics[\"AUC\"]=metrics.roc_auc_score(y_true, predicted_prob)\n",
    "    \n",
    "    # metrics based on the predicted labels.\n",
    "    result_metrics[\"accuracy\"]=metrics.accuracy_score(y_true, predicted)\n",
    "    result_metrics[\"recall\"]=metrics.recall_score(y_true, predicted)\n",
    "    result_metrics[\"precision\"]=metrics.precision_score(y_true, predicted)\n",
    "    result_metrics[\"F1\"]=metrics.f1_score(y_true, predicted)\n",
    "       \n",
    "    return result_metrics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(trained_model_path, test_X, test_Y, save_results_path):\n",
    "    \"\"\"\n",
    "    Predict the label with the trained model.\n",
    "    \"\"\"\n",
    "    #make dir.\n",
    "    if not os.path.exists(save_results_path):\n",
    "        os.makedirs(save_results_path)\n",
    "        \n",
    "    # load the model.\n",
    "    model = joblib.load(trained_model_path)\n",
    "    \n",
    "    # predict.\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        predicted_prob = model.predict_proba(test_X)[:,1]\n",
    "    else:\n",
    "        predicted_prob = model.predict(test_X)\n",
    "    \n",
    "    #save the predicted probability.\n",
    "    predicted_prob_df=pd.DataFrame(data=predicted_prob, columns=['predicted_prob'], index=test_X.index)\n",
    "    predicted_prob_df.to_csv(os.path(save_results_path, \"predicted_prob.csv\"), line_terminator='\\n')\n",
    "    \n",
    "    #calculate the evaluation metrics.\n",
    "    result_metrics=None\n",
    "    if test_Y is not None:\n",
    "        #ROC curve\n",
    "        plot_ROC_curve(test_Y, predicted_prob, save_results_path)\n",
    "\n",
    "        #explore the threshold\n",
    "        threshold=select_threshold(test_Y, predicted_prob, save_results_path)\n",
    "        save_log(\"\\nThe threshold which maximize the F1 score is: {}.\".format(threshold))\n",
    "\n",
    "        #define the threshold\n",
    "        predicted = predicted_prob > threshold\n",
    "\n",
    "        #plot confusion matrix\n",
    "        plot_confusion_matrix(test_Y, predicted, save_results_path)\n",
    "        \n",
    "        #calculate and save metrics \n",
    "        result_metrics=calculate_metrics(test_Y, predicted, predicted_prob)\n",
    "        save_dict(result_metrics, os.path.join(save_results_path, \"prediction_metrics.txt\"))\n",
    "        save_log(\"Prediction results on test set:\\n{}\".format(result_metrics))\n",
    "        \n",
    "    return result_metrics\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function for binary classification: train and predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_binary_classification_train(train_data, feature_columns, label_column, save_results_path, feature_selection_type):\n",
    "    \"\"\"\n",
    "    Find the best model from a list of models, and retrained it on the whole training dataset.\n",
    "    \"\"\"\n",
    "    save_log(\"****** Begin to find and train the best model to predict {} ....... ******\".format(label_column))\n",
    "    \n",
    "    ## Data preprocessing.\n",
    "    train_X=train_data[feature_columns]\n",
    "    train_Y=train_data[label_column]\n",
    "    save_log(\"\\n-train_data.shape={} \\n-len(feature_columns)={} \\n-label_column={}\".format(train_data.shape, len(feature_columns), label_column))\n",
    "\n",
    "    #Step 1: find the best hyperparameters.\n",
    "    best_model_name=main_find_best_model(train_X, train_Y, save_results_path, feature_selection_type)\n",
    "        \n",
    "    #Step 2: retrain the selected best model on the whole training dataset.\n",
    "    trained_model_path=retrain_the_best_model(train_X, train_Y, best_model_name, save_results_path)\n",
    "    \n",
    "    return trained_model_path\n",
    "\n",
    "    \n",
    "def perform_binary_classification_predict(trained_model_path, test_data_dict, feature_columns, label_column, save_results_path):\n",
    "    \"\"\"\n",
    "    make predictions with the trained best model.\n",
    "    \"\"\"\n",
    "    save_results_path=os.path.dirname(trained_model_path)\n",
    "    \n",
    "    for description, test_data in test_data_dict.items():\n",
    "        save_log(\"\\n- Predict for {}: \\n-test_data.shape={} \\n-len(feature_columns)={} \\n-label_column={}\".format(description, test_data.shape, len(feature_columns), label_column))\n",
    "        test_X=test_data[feature_columns]\n",
    "        test_Y=test_data[label_column] if label_column in test_data.columns else None\n",
    "\n",
    "        predict(trained_model_path, test_X, test_Y, save_results_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main: call the function and perform the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(task_name, task_settings):\n",
    "    save_log(\"\\n =================== task_name={} =============== \\n - task_settings={}\".format(task_name, task_settings))\n",
    "    \n",
    "    #read the settings\n",
    "    train_data=task_settings[\"train_data\"]\n",
    "    test_data_dict=task_settings[\"test_data_dict\"]\n",
    "    feature_columns=task_settings[\"feature_columns\"]\n",
    "    label_column=task_settings[\"label_column\"]\n",
    "    base_results_path=task_settings[\"base_results_path\"]\n",
    "    feature_selection_type=get_basic_settings()[\"feature_selection_method\"]\n",
    "    \n",
    "    # create the folder to save results.\n",
    "    save_results_path=os.path.join(base_results_path, task_name)\n",
    "    if not os.path.exists(save_results_path):\n",
    "        os.makedirs(save_results_path) \n",
    "    \n",
    "    ## train the model\n",
    "    trained_model_path=perform_binary_classification_train(train_data, feature_columns, label_column, save_results_path, feature_selection_type)\n",
    "    \n",
    "    ## make predictions\n",
    "    test_data_dict=dict(**{\"train_data\":train_data}, **test_data_dict)    \n",
    "    perform_binary_classification_predict(trained_model_path, test_data_dict, feature_columns, label_column)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "task_name, task_settings=get_classification_task_settings()\n",
    "main(task_name, task_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
